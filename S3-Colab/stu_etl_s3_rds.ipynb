{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud ETL with S3, PySpark, and RDS\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* As the sole data person at your new company, you have been tasked with cleaning the data from an Excel spreadsheet—which has been exported as a CSV—and creating an SQL database with this data. In other words, you will be performing ETL.\n",
    "\n",
    "* Take a moment to review the SQL table schemata, which reflect the requirements for the company's new database. Your company will be using S3 for file storage and RDS to host SQL databases.\n",
    "\n",
    "* Then, complete the following tasks:\n",
    "\n",
    "  * Upload the *employees.csv* file in the *Resources* folder to S3. **Note:** be sure to make the S3 bucket public.\n",
    "\n",
    "  * Upload the unsolved jupyter notebook to Colab and use Spark to clean and transform the data.\n",
    "\n",
    "  * With the *schema.sql* file from the *Resources* folder, use pgAdmin to create the table schemata in RDS.\n",
    "\n",
    "  * Load the data from Pandas DataFrames into RDS.\n",
    "\n",
    "### Attribution\n",
    "\n",
    "The dataset was sourced from [https://github.com/Microsoft/sql-server-samples](https://github.com/Microsoft/sql-server-samples).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Java, Spark, and Findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y6pz7LGh_L1p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: apt-get: command not found\n"
     ]
    }
   ],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the PostgreSQL driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y6pz7LGh_L1p"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y6pz7LGh_L1p"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pySpark app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the *employee.csv* data from AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIGU4Tzs_Q4g"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "# Load in employee.csv from S3 into a DataFrame\n",
    "url = \"https://<bucket name>.s3.amazonaws.com/employee.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "df = spark.read.option('header', 'true').csv(SparkFiles.get(\"employee.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kdtKPZ1w_V3e"
   },
   "source": [
    "### Drop duplicates and incomplete rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "10qoqQv3_Y_w"
   },
   "outputs": [],
   "source": [
    "print(df.count())\n",
    "df = df.dropna()\n",
    "print(df.count())\n",
    "df = df.dropDuplicates()\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GAgvBYkG_awE"
   },
   "source": [
    "### Display the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SXtPi-Hw_dCe"
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klopnaUE_eZV"
   },
   "source": [
    "### Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9b0dvAat_f7L"
   },
   "outputs": [],
   "source": [
    "df1 = df.withColumnRenamed(\"Employee ID\", \"employee_id\") \\\n",
    "        .withColumnRenamed(\"Email\", \"email\") \\\n",
    "        .withColumnRenamed(\"Gender\", \"gender\") \\\n",
    "        .withColumnRenamed(\"Hire Date\", \"hire_date\") \\\n",
    "        .withColumnRenamed(\"DOB\", \"dob\") \\\n",
    "        .withColumnRenamed(\"Encrypted Password\", \"password\")\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AbG7CtSA_gfw"
   },
   "source": [
    "### Create a new DataFrame for employee info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "saJ1WiZb_ik6"
   },
   "outputs": [],
   "source": [
    "employee_personal_info = df1.select([\"employee_id\", \"email\", \"gender\", \"hire_date\", \"dob\"])\n",
    "employee_personal_info.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Or1RnPr_nzd"
   },
   "source": [
    "### Create a new DataFrame for employee passwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7mDI_Xg9_q8_"
   },
   "outputs": [],
   "source": [
    "employee_password = df1.select([\"employee_id\", \"password\"])\n",
    "employee_password.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PDvBiVN_jGe"
   },
   "source": [
    "### Configure settings for RDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fOIS3viE_leJ"
   },
   "outputs": [],
   "source": [
    "mode=\"append\"\n",
    "jdbc_url = \"jdbc:postgresql://<insert endpoint>:5432/my_data_class_db\"\n",
    "config = {\"user\":\"root\",\n",
    "          \"password\": \"<insert password>\",\n",
    "          \"driver\":\"org.postgresql.Driver\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the cleaned employee data to the *employee_personal_info* table in RDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K6_O84CK_nV5"
   },
   "outputs": [],
   "source": [
    "employee_personal_info.write.jdbc(url=jdbc_url, table='employee_personal_info', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCmoHROm_saU"
   },
   "source": [
    "### Write the employee password DataFrame to the *employee_password* table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNKz5FHg_uV9"
   },
   "outputs": [],
   "source": [
    "employee_password.write.jdbc(url=jdbc_url, table='employee_password', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tvw--Ool_vju"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "solved_stu_etl_s3_rds.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
